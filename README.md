<div align="center">
  <div>
    <h2>Hey there ğŸ‘‹ I'm Abhishek</h2>
    <p><em>Obsessed with making models go brrr â€” from training to real-time inference at scale</em></p>
  </div>
  <img src="https://media.giphy.com/media/dWesBcTLavkZuG35MI/giphy.gif" width="600" height="300"/>
  <div id="badges">
   <a href="https://www.linkedin.com/in/enterprise-cloud-architect">
     <img src="https://img.shields.io/badge/LinkedIn-blue?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn Badge"/>
   </a>
   <a href="https://twitter.com/AbhiForTweeting">
     <img src="https://img.shields.io/badge/Twitter-blue?style=for-the-badge&logo=twitter&logoColor=white" alt="Twitter Badge"/>
   </a>
  </div>
</div>

---

### âš¡ About Me

- ğŸ”¥ I live and breathe **AI Inference** â€” optimizing models to run faster, cheaper, and at massive scale
- ğŸ§  Deep in the **NVIDIA inference stack**: TensorRT, Triton Inference Server, CUDA, TensorRT-LLM, and NIM
- ğŸš€ Passionate about squeezing every last TFLOP out of GPUs â€” from A100s to H100s to Blackwell
- ğŸ—ï¸ Building and scaling **inference pipelines** that serve millions of requests with minimal latency
- ğŸŒ Background in cloud-native architecture across **AWS, Azure, and GCP** â€” now laser-focused on GPU-accelerated inference infrastructure
- ğŸ¤ Open to collaborating on **open-source inference tooling**, model optimization, and high-performance serving systems

---

[![GitHub Streak](http://github-readme-streak-stats.herokuapp.com?user=abhiongithub&theme=dark&background=000000)](https://git.io/streak-stats)

[![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=abhiongithub&layout=compact&theme=dark)](https://github.com/abhiongithub/github-readme-stats)

<picture>
<source 
  srcset="https://github-readme-stats.vercel.app/api?username=abhiongithub&show_icons=true&theme=dark"
  media="(prefers-color-scheme: dark)"
/>
<source
  srcset="https://github-readme-stats.vercel.app/api?username=abhiongithub&show_icons=true"
  media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
/>
<img src="https://github-readme-stats.vercel.app/api?username=abhiongithub&show_icons=true" />
</picture>

---

<h3 align="left">ğŸ› ï¸ Inference & AI Stack:</h3>
<p align="left">
  <img src="https://img.shields.io/badge/NVIDIA-TensorRT-76B900?style=flat-square&logo=nvidia&logoColor=white" alt="TensorRT"/>
  <img src="https://img.shields.io/badge/NVIDIA-Triton-76B900?style=flat-square&logo=nvidia&logoColor=white" alt="Triton"/>
  <img src="https://img.shields.io/badge/NVIDIA-CUDA-76B900?style=flat-square&logo=nvidia&logoColor=white" alt="CUDA"/>
  <img src="https://img.shields.io/badge/NVIDIA-TensorRT--LLM-76B900?style=flat-square&logo=nvidia&logoColor=white" alt="TensorRT-LLM"/>
  <img src="https://img.shields.io/badge/NVIDIA-NIM-76B900?style=flat-square&logo=nvidia&logoColor=white" alt="NIM"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/C++-00599C?style=flat-square&logo=cplusplus&logoColor=white" alt="C++"/>
  <img src="https://img.shields.io/badge/Go-00ADD8?style=flat-square&logo=go&logoColor=white" alt="Go"/>
  <img src="https://img.shields.io/badge/Rust-000000?style=flat-square&logo=rust&logoColor=white" alt="Rust"/>
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white" alt="PyTorch"/>
  <img src="https://img.shields.io/badge/vLLM-FF6F00?style=flat-square" alt="vLLM"/>
  <img src="https://img.shields.io/badge/Docker-2496ED?style=flat-square&logo=docker&logoColor=white" alt="Docker"/>
  <img src="https://img.shields.io/badge/Kubernetes-326CE5?style=flat-square&logo=kubernetes&logoColor=white" alt="Kubernetes"/>
</p>

<h3 align="left">â˜ï¸ Cloud & Infra:</h3>
<p align="left">
  <img src="https://www.vectorlogo.zone/logos/amazon_aws/amazon_aws-icon.svg" alt="aws" width="40" height="40"/>
  <img src="https://www.vectorlogo.zone/logos/microsoft_azure/microsoft_azure-icon.svg" alt="azure" width="40" height="40"/>
  <img src="https://www.vectorlogo.zone/logos/google_cloud/google_cloud-icon.svg" alt="gcp" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/docker/docker-original-wordmark.svg" alt="docker" width="40" height="40"/>
  <img src="https://www.vectorlogo.zone/logos/kubernetes/kubernetes-icon.svg" alt="kubernetes" width="40" height="40"/>
  <img src="https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg" alt="git" width="40" height="40"/>
</p>

---

- ğŸ’¬ Ask me about **GPU-accelerated inference**, model optimization, batching strategies, and scaling LLM serving
- ğŸ‘¯ Looking to collaborate on **inference engines, model compilers, and open-source AI infrastructure**

<details>
 <summary><strong>âš¡ What I'm focused on in 2025â€“2026</strong></summary>
   <br/>
   - Optimizing <strong>LLM inference</strong> â€” KV-cache management, speculative decoding, continuous batching<br/>
   - <strong>TensorRT-LLM</strong> and <strong>TensorRT</strong> for maximum throughput on NVIDIA GPUs<br/>
   - <strong>Triton Inference Server</strong> â€” model ensembles, dynamic batching, multi-GPU serving<br/>
   - <strong>NVIDIA NIM</strong> microservices for production-grade AI deployment<br/>
   - <strong>CUDA</strong> kernel optimization and custom inference operators<br/>
   - <strong>vLLM</strong>, <strong>SGLang</strong>, and other open-source LLM serving frameworks<br/>
   - Multi-node inference on <strong>H100 / Blackwell</strong> clusters with NVLink & NVSwitch<br/>
   - <strong>Quantization</strong> (FP8, INT4, AWQ, GPTQ) for efficient model deployment<br/>
   - Go, Rust, and C++ for high-performance inference infrastructure<br/>
</details>

<details>
 <summary><strong>ğŸ§  Technologies I know</strong></summary>
   <br/>
   - <strong>Inference:</strong> TensorRT, TensorRT-LLM, Triton Inference Server, NVIDIA NIM, vLLM, ONNX Runtime<br/>
   - <strong>GPU/Compute:</strong> CUDA, cuDNN, NCCL, NVLink, Multi-Instance GPU (MIG)<br/>
   - <strong>ML Frameworks:</strong> PyTorch, JAX, ONNX<br/>
   - <strong>Cloud:</strong> AWS (SageMaker, EKS, EC2 P/G instances), Azure (AKS, NC/ND VMs), GCP (GKE, A3/A2 VMs)<br/>
   - <strong>Containers & Orchestration:</strong> Docker, Kubernetes, Helm, NVIDIA GPU Operator<br/>
   - <strong>Languages:</strong> Python, C++, Go, Rust, C#, Java<br/>
   - <strong>IaC:</strong> Terraform, Pulumi, AWS CloudFormation, Azure ARM<br/>
   - <strong>Monitoring:</strong> Prometheus, Grafana, Splunk, Elastic Stack<br/>
   - <strong>Streaming:</strong> Apache Kafka, Apache Flink, Spark Streaming<br/>
</details>

<details>
 <summary><strong>ğŸ“š Previously</strong></summary>
   <br/>
   - Cloud-native architecture and distributed systems across AWS & Azure<br/>
   - Serverless and modular monolithic architectures<br/>
   - Full-stack development with C#/.NET, Java/Spring Boot, React<br/>
   - GoLang microservices (GoORM, Fiber, Chi, Mux)<br/>
   - Distributed Application Runtime (DAPR)<br/>
   - Cross-platform development with Xamarin/MAUI<br/>
</details>
